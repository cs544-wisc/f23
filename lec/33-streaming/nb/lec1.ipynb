{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea04583-214c-4547-a346-24211910e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -m grpc_tools.protoc -I=. --python_out=. animals.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12accd7e-ea68-4895-9650-24d3e601456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animals_pb2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63fe0d9d-e254-44ff-bff6-9c63d51b462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d28414-f67d-476b-8261-21b9defbc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = \"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c9d8673-f7ec-4c6b-890e-6e1963776b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = KafkaAdminClient(bootstrap_servers=[broker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df2c0eb0-15c2-4ec5-9783-d2e355e44d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4327cc6-901f-499c-94a0-cbb273c75b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    admin.create_topics([NewTopic(\"animals\", 4, 1)]) # protobufs\n",
    "except TopicAlreadyExistsError:\n",
    "    pass\n",
    "try:\n",
    "    admin.create_topics([NewTopic(\"animals-json\", 4, 1)]) # json\n",
    "except TopicAlreadyExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861305e1-71d0-4cd1-ac8a-247745bb00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=[broker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8fc2b41-acf7-424d-b3ff-8de8a485c90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7f08fc140520>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = \"A\"\n",
    "value = Sighting(beach=key, animal=\"shark\").SerializeToString()\n",
    "producer.send(\"animals\", value, bytes(key, \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b604fccb-08f2-4b20-ab32-a527d5ab5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, threading\n",
    "\n",
    "def animal_producer():\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "\n",
    "        value = Sighting(beach=beach, animal=animal).SerializeToString()\n",
    "        producer.send(\"animals\", value, bytes(beach, \"utf-8\"))\n",
    "        time.sleep(1)\n",
    "threading.Thread(target=animal_producer).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e888ee4-6e7b-495c-bb6a-160fa1a4926d",
   "metadata": {},
   "source": [
    "# Streaming Group BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb5d3f5-612a-4162-b08e-4ce5e9c0755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Lock\n",
    "\n",
    "lock = Lock()\n",
    "def Print(*args):\n",
    "    with lock:\n",
    "        print(*args)\n",
    "\n",
    "Print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28983c4f-5925-46c2-8c5e-260299154a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import TopicPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97dcd6e6-4903-4718-be97-35a47ead78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TopicPartition(topic='animals', partition=0), TopicPartition(topic='animals', partition=1)]\n",
      "[TopicPartition(topic='animals', partition=2), TopicPartition(topic='animals', partition=3)]\n"
     ]
    }
   ],
   "source": [
    "def beach_consumer(parts=[]):\n",
    "    counts = {}  # key=beach, value=count\n",
    "    partitions = [TopicPartition(\"animals\", p) for p in parts]\n",
    "    print(partitions)\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign(partitions)\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "                if not s.beach in counts:\n",
    "                    counts[s.beach] = 0\n",
    "                counts[s.beach] += 1\n",
    "        Print(parts, counts)\n",
    "\n",
    "threading.Thread(target=beach_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=beach_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80354fec-af78-485c-9dcb-31d3a826a693",
   "metadata": {},
   "source": [
    "# Another Group BY, but not by the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "878c6117-4816-488e-b8bc-5e3458e97f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TopicPartition(topic='animals', partition=0), TopicPartition(topic='animals', partition=1)]\n",
      "[TopicPartition(topic='animals', partition=2), TopicPartition(topic='animals', partition=3)]\n"
     ]
    }
   ],
   "source": [
    "def animal_consumer(parts=[]):\n",
    "    counts = {}  # key=animal, value=count\n",
    "    partitions = [TopicPartition(\"animals\", p) for p in parts]\n",
    "    print(partitions)\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign(partitions)\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "                if not s.animal in counts:\n",
    "                    counts[s.animal] = 0\n",
    "                counts[s.animal] += 1\n",
    "        Print(parts, counts)\n",
    "\n",
    "threading.Thread(target=animal_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=animal_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f5d3a-4316-4133-b60a-29308667ec7a",
   "metadata": {},
   "source": [
    "# Spark Streaming Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3e72a2d-41bc-4a79-b817-a6213bc50761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1] {'shark': 1}\n",
      "[2, 3] {'shark': 1}\n",
      "[0, 1] {'C': 1}\n",
      "[2, 3] {'A': 1}\n",
      "[0, 1] {'C': 1, 'B': 1}\n",
      "[0, 1] {'shark': 1, 'seagull': 1}\n",
      "[2, 3] {'shark': 1}\n",
      "[2, 3] {'A': 1}\n",
      "[0, 1] {'C': 1, 'B': 1}\n",
      "[0, 1] {'shark': 1, 'seagull': 1}\n",
      "[0, 1] {'C': 2, 'B': 1}\n",
      "[0, 1] {'shark': 2, 'seagull': 1}\n",
      "[2, 3] {'shark': 1}\n",
      "[2, 3] {'A': 1}\n",
      "[2, 3] {'shark': 1, 'seagull': 1}\n",
      "[2, 3] {'A': 1, 'H': 1}\n",
      "[0, 1] {'C': 2, 'B': 1}\n",
      "[0, 1] {'shark': 2, 'seagull': 1}\n",
      "[0, 1] {'C': 2, 'B': 1, 'D': 1}\n",
      "[2, 3] {'shark': 1, 'seagull': 1}\n",
      "[0, 1] {'shark': 3, 'seagull': 1}\n",
      "[2, 3] {'A': 1, 'H': 1}\n",
      "[0, 1] {'C': 2, 'B': 1, 'D': 1}\n",
      "[0, 1] {'C': 2, 'B': 2, 'D': 1}\n",
      "[2, 3] {'shark': 1, 'seagull': 1}\n",
      "[0, 1] {'shark': 3, 'seagull': 1}\n",
      "[0, 1] {'shark': 3, 'seagull': 1, 'turtle': 1}\n",
      "[2, 3] {'A': 1, 'H': 1}\n",
      "[0, 1] {'C': 3, 'B': 2, 'D': 1}\n",
      "[0, 1] {'shark': 4, 'seagull': 1, 'turtle': 1}\n",
      "[2, 3] {'shark': 1, 'seagull': 1}\n",
      "[2, 3] {'A': 1, 'H': 1}\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "[0, 1] {'C': 3, 'B': 2, 'D': 1}\n",
      "[0, 1] {'shark': 4, 'seagull': 1, 'turtle': 1}\n",
      "[2, 3] {'shark': 1, 'seagull': 1, 'turtle': 1}\n",
      "[2, 3] {'A': 2, 'H': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-56d064e8-4f20-48a5-9fc1-48b00e65181d;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3] {'shark': 1, 'seagull': 1, 'turtle': 1}\n",
      "[2, 3] {'A': 2, 'H': 1}\n",
      "[2, 3] {'shark': 1, 'seagull': 1, 'turtle': 1}\n",
      "[2, 3] {'A': 2, 'H': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (79ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (51ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (303ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (33ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (568ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (45ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (66ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (49ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (329ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (29ms)\n",
      ":: resolution report :: resolve 5444ms :: artifacts dl 1620ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-56d064e8-4f20-48a5-9fc1-48b00e65181d\n",
      "\tconfs: [default]\n",
      "\t11 artifacts copied, 0 already retrieved (56767kB/190ms)\n",
      "23/11/22 15:36:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Spark session (with Kafka jar)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"demo\")\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "124383d7-8d69-4655-b3a0-0fb138c65823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, threading, json\n",
    "\n",
    "def animal_json_producer():\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "\n",
    "        #value = Sighting(beach=beach, animal=animal).SerializeToString()\n",
    "        value = bytes(json.dumps({\"beach\": beach, \"animal\": animal}), \"utf-8\")\n",
    "        producer.send(\"animals-json\", value, bytes(beach, \"utf-8\"))\n",
    "        time.sleep(1)\n",
    "threading.Thread(target=animal_json_producer).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44db0d2b-1a6a-4dc7-8159-7de8a9d965da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    " spark.read.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", broker)\n",
    " .option(\"subscribe\", \"animals-json\")\n",
    " .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7619f68e-abd9-401e-8f76-abe0047031ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'binary'),\n",
       " ('value', 'binary'),\n",
       " ('topic', 'string'),\n",
       " ('partition', 'int'),\n",
       " ('offset', 'bigint'),\n",
       " ('timestamp', 'timestamp'),\n",
       " ('timestampType', 'int')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e3ce7e5-91b9-439e-884d-6117a314e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 15:37:08 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2de075fc-fd0a-4871-b941-5ca45162e8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 15:37:20 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>partition</th>\n",
       "      <th>offset</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestampType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[67]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-22 15:37:01.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-22 15:37:03.285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-11-22 15:37:04.287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-11-22 15:37:05.288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[67]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-11-22 15:37:07.291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    key                                              value         topic  \\\n",
       "0  [67]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "1  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "2  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "3  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "4  [67]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "\n",
       "   partition  offset               timestamp  timestampType  \n",
       "0          0       0 2023-11-22 15:37:01.273              0  \n",
       "1          0       1 2023-11-22 15:37:03.285              0  \n",
       "2          0       2 2023-11-22 15:37:04.287              0  \n",
       "3          0       3 2023-11-22 15:37:05.288              0  \n",
       "4          0       4 2023-11-22 15:37:07.291              0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a7c606a-125a-41b9-ad02-749f60160760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "356d24b4-c456-4c43-b137-f7af099a9c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 15:37:22 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>beach</th>\n",
       "      <th>animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>shark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>dolphin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key beach   animal\n",
       "0   C     C   turtle\n",
       "1   B     B    shark\n",
       "2   B     B  seagull\n",
       "3   B     B  seagull\n",
       "4   C     C  dolphin"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "animals = (df\n",
    " .select(\n",
    "     col(\"key\").cast(\"string\"),\n",
    "     col(\"value\").cast(\"string\")\n",
    " )\n",
    " .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    " .select(\"key\", \"value.*\")\n",
    ")\n",
    "animals.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da04cb3c-a47a-4922-877a-90278aa4dfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f704ae2-a253-44b6-8a8f-9bff42337093",
   "metadata": {},
   "source": [
    "# Let's make it a streaming DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acd5da3a-11da-4676-9ac2-4cedb375fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    " spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", broker)\n",
    " .option(\"subscribe\", \"animals-json\")\n",
    " .option(\"startingOffsets\", \"earliest\")\n",
    " .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4802bb6b-4613-464d-9717-53c2599e8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "animals = (df\n",
    " .select(\n",
    "     col(\"key\").cast(\"string\"),\n",
    "     col(\"value\").cast(\"string\")\n",
    " )\n",
    " .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    " .select(\"key\", \"value.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3ea9dc1-3860-4714-b467-02dc2acbb11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89803550-61de-4a4f-bbda-cff31bfe8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work for streaming\n",
    "# animals.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1642ed2-3ba3-4377-8bdb-282bbf2cef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark streaming\n",
    "# source => transformations => sink\n",
    "\n",
    "# spark.readStream(????).?????.writeStream(????)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850896d6-1cc3-49a2-941d-d2864b0f233b",
   "metadata": {},
   "source": [
    "# Shark Alert Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "916d3f8a-dda2-41f8-956c-517cd93c316d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 15:37:24 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2143b102-2161-4b08-a16d-60e633776e4d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/22 15:37:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.streaming.query.StreamingQuery"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = (\n",
    " animals.filter(\"animal = 'shark'\")\n",
    " .writeStream.format(\"console\")\n",
    " .trigger(processingTime=\"5 seconds\")\n",
    " .outputMode(\"append\")\n",
    ").start()\n",
    "type(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c1cc576-9a1f-451a-83c9-852d4ae3d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()\n",
    "# spark.streams.active[0].stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cad408-825a-458b-967b-8472655bcba0",
   "metadata": {},
   "source": [
    "# Streaming GROUP BY on animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "831a72e6-00b6-4855-bac0-1c3658f25ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 15:59:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9dc99422-d06d-4b78-a180-97519cec00d6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/22 15:59:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/22 15:59:19 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 76:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  319|\n",
      "|dolphin|  331|\n",
      "|seagull|  346|\n",
      "| turtle|  340|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  321|\n",
      "|seagull|  346|\n",
      "|dolphin|  331|\n",
      "| turtle|  341|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  322|\n",
      "|seagull|  347|\n",
      "|dolphin|  331|\n",
      "| turtle|  342|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  324|\n",
      "|seagull|  348|\n",
      "|dolphin|  331|\n",
      "| turtle|  344|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  325|\n",
      "|seagull|  348|\n",
      "|dolphin|  334|\n",
      "| turtle|  345|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = (\n",
    " animals.groupby(\"animal\").count()\n",
    " .writeStream.format(\"console\")\n",
    " .trigger(processingTime=\"5 seconds\")\n",
    " .outputMode(\"complete\")\n",
    ").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f9a6115-182e-4a62-9f47-a8c16ef7cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35116382-9011-45d9-96cd-d17c70287ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
