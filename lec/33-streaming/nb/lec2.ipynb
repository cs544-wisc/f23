{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e12da1-6216-41e1-beec-eae2d52c4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -m grpc_tools.protoc -I=. --python_out=. animals.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e74c81-d584-4504-9ef5-2c89227d3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animals_pb2 import Sighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0f722f-060b-4236-b28c-b8cd86a6bc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beach: \"A\"\n",
       "animal: \"shark\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sighting(animal=\"shark\", beach=\"A\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5daff3-d47f-433f-8933-e074145b1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x01A\\x12\\x05shark'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60cea48-9cd2-4635-a822-b5a8992f6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1a5535-36c1-416c-8d70-7493dbf26338",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = \"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdec839a-2f6b-4e41-8d79-05271c12fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = KafkaAdminClient(bootstrap_servers=[broker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b0baaa-8ddc-41d3-9091-851009b672bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import NewTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7243d8c5-cdb0-4bee-8b0e-1c77a610fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.errors import TopicAlreadyExistsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "207f3f33-3edd-4039-9ed2-6a606e854e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    admin.create_topics([NewTopic(\"animals\", 4, 1)])   # protobufs\n",
    "except TopicAlreadyExistsError:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    admin.create_topics([NewTopic(\"animals-json\", 4, 1)])   # JSON\n",
    "except TopicAlreadyExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf52c987-d4c2-4077-a72b-aada380c81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, threading\n",
    "\n",
    "def animal_gen():\n",
    "    producer = KafkaProducer(bootstrap_servers=[broker])\n",
    "\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "        s = Sighting(animal=animal, beach=beach)\n",
    "        producer.send(\"animals\", value=s.SerializeToString(), key=bytes(beach, \"utf-8\"))\n",
    "        time.sleep(1)\n",
    "\n",
    "threading.Thread(target=animal_gen).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e042e6-56ec-41c6-bf38-032b7bb899fb",
   "metadata": {},
   "source": [
    "# Streaming Group By (count occurences per beach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e7199cf-5c69-40fd-9794-721649a71f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Lock\n",
    "\n",
    "lock = Lock()\n",
    "def Print(*args):\n",
    "    with lock:\n",
    "        print(*args)\n",
    "\n",
    "Print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ba5b05-d29f-42cb-9d35-161a74e149a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import TopicPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bfd1425-ae07-4c09-85d4-e56ad659abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beach_consumer(partitions=[]):\n",
    "    counts = {}   # key=beach, value=count\n",
    "    \n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign([TopicPartition(\"animals\", p) for p in partitions])\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):      # TODO: loop forever\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "\n",
    "                if not s.beach in counts:\n",
    "                    counts[s.beach] = 0\n",
    "                counts[s.beach] += 1\n",
    "        Print(partitions, counts)\n",
    "threading.Thread(target=beach_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=beach_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96189610-b7c7-4f65-bd7f-850b3dcc6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animal_consumer(partitions=[]):\n",
    "    counts = {}   # key=animal, value=count\n",
    "    \n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign([TopicPartition(\"animals\", p) for p in partitions])\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):      # TODO: loop forever\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "\n",
    "                if not s.animal in counts:\n",
    "                    counts[s.animal] = 0\n",
    "                counts[s.animal] += 1\n",
    "        Print(partitions, counts)\n",
    "threading.Thread(target=animal_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=animal_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b0bcc-ab55-40a5-a186-4622ea6f49ff",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c206aa-56e1-4aad-b631-85c9cea0672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, threading, json\n",
    "\n",
    "def animal_gen_json():\n",
    "    producer = KafkaProducer(bootstrap_servers=[broker])\n",
    "\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "\n",
    "        value = bytes(json.dumps({\"beach\": beach, \"animal\": animal}), \"utf-8\")\n",
    "        producer.send(\"animals-json\", value=value, key=bytes(beach, \"utf-8\"))\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "threading.Thread(target=animal_gen_json).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8339f693-ab99-4e4e-8040-63033079e5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1] {'shark': 129, 'dolphin': 132, 'seagull': 128, 'turtle': 111}\n",
      "[0, 1] {'shark': 260, 'dolphin': 266, 'seagull': 253, 'turtle': 221}\n",
      "[0, 1] {'shark': 405, 'dolphin': 380, 'seagull': 379, 'turtle': 336}\n",
      "[0, 1] {'shark': 536, 'dolphin': 492, 'seagull': 510, 'turtle': 462}\n",
      "[0, 1] {'shark': 650, 'dolphin': 619, 'seagull': 639, 'turtle': 592}\n",
      "[2, 3] {'H': 260, 'G': 240}\n",
      "[2, 3] {'H': 508, 'G': 492}\n",
      "[2, 3] {'H': 736, 'G': 764}\n",
      "[2, 3] {'H': 979, 'G': 1021}\n",
      "[2, 3] {'H': 1229, 'G': 1271}\n",
      "[2, 3] {'shark': 127, 'turtle': 128, 'dolphin': 125, 'seagull': 120}\n",
      "[2, 3] {'shark': 226, 'turtle': 235, 'dolphin': 273, 'seagull': 266}\n",
      "[2, 3] {'shark': 339, 'turtle': 366, 'dolphin': 401, 'seagull': 394}\n",
      "[2, 3] {'shark': 456, 'turtle': 471, 'dolphin': 561, 'seagull': 512}\n",
      "[2, 3] {'shark': 576, 'turtle': 597, 'dolphin': 684, 'seagull': 643}\n",
      "[0, 1] {'C': 181, 'B': 155, 'I': 164}\n",
      "[0, 1] {'C': 336, 'B': 321, 'I': 343}\n",
      "[0, 1] {'C': 518, 'B': 476, 'I': 506}\n",
      "[0, 1] {'C': 698, 'B': 633, 'I': 669}\n",
      "[0, 1] {'C': 862, 'B': 796, 'I': 842}\n",
      "[0, 1] {'C': 1016, 'B': 987, 'I': 997}\n",
      "[0, 1] {'C': 1186, 'B': 1152, 'I': 1162}\n",
      "[0, 1] {'C': 1341, 'B': 1324, 'I': 1335}\n",
      "[2, 3] {'H': 1429, 'G': 1479, 'A': 43, 'F': 49}\n",
      "[2, 3] {'H': 1429, 'G': 1479, 'A': 287, 'F': 305}\n",
      "[2, 3] {'H': 1429, 'G': 1479, 'A': 528, 'F': 564}\n",
      "[2, 3] {'H': 1429, 'G': 1479, 'A': 759, 'F': 833}\n",
      "[2, 3] {'H': 1429, 'G': 1479, 'A': 1009, 'F': 1083}\n",
      "[0, 1] {'shark': 777, 'dolphin': 746, 'seagull': 775, 'turtle': 702}\n",
      "[0, 1] {'shark': 909, 'dolphin': 864, 'seagull': 910, 'turtle': 817}\n",
      "[0, 1] {'shark': 1052, 'dolphin': 983, 'seagull': 1030, 'turtle': 935}\n",
      "[0, 1] {'shark': 1164, 'dolphin': 1105, 'seagull': 1161, 'turtle': 1070}\n",
      "[0, 1] {'shark': 1282, 'dolphin': 1231, 'seagull': 1293, 'turtle': 1194}\n",
      "[0, 1] {'C': 1471, 'B': 1435, 'I': 1452, 'D': 76, 'E': 66}\n",
      "[0, 1] {'C': 1471, 'B': 1435, 'I': 1452, 'D': 339, 'E': 303}\n",
      "[2, 3] {'shark': 712, 'turtle': 711, 'dolphin': 804, 'seagull': 773}\n",
      "[2, 3] {'shark': 842, 'turtle': 846, 'dolphin': 916, 'seagull': 896}\n",
      "[2, 3] {'shark': 979, 'turtle': 966, 'dolphin': 1042, 'seagull': 1013}\n",
      "[2, 3] {'shark': 1101, 'turtle': 1089, 'dolphin': 1190, 'seagull': 1120}\n",
      "[2, 3] {'shark': 1223, 'turtle': 1208, 'dolphin': 1310, 'seagull': 1259}\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-899f1142-cf3d-4da9-bd48-d51da8ff492c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1440ms :: artifacts dl 58ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-899f1142-cf3d-4da9-bd48-d51da8ff492c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/32ms)\n",
      "23/11/22 19:13:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/22 19:13:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Spark session (with Kafka jar)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"demo\")\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70f52414-df72-48cb-b851-254a12739ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", broker)\n",
    "    .option(\"subscribe\", \"animals-json\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20e78ed8-3587-4789-9e51-9914158513c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'binary'),\n",
       " ('value', 'binary'),\n",
       " ('topic', 'string'),\n",
       " ('partition', 'int'),\n",
       " ('offset', 'bigint'),\n",
       " ('timestamp', 'timestamp'),\n",
       " ('timestampType', 'int')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb6f3dd4-3bdc-4f2a-b89d-93e673cf7f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 19:13:25 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>partition</th>\n",
       "      <th>offset</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestampType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[67]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-22 15:37:01.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-22 15:37:03.285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-11-22 15:37:04.287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-11-22 15:37:05.288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[67]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-11-22 15:37:07.291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    key                                              value         topic  \\\n",
       "0  [67]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "1  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "2  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "3  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "4  [67]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "\n",
       "   partition  offset               timestamp  timestampType  \n",
       "0          0       0 2023-11-22 15:37:01.273              0  \n",
       "1          0       1 2023-11-22 15:37:03.285              0  \n",
       "2          0       2 2023-11-22 15:37:04.287              0  \n",
       "3          0       3 2023-11-22 15:37:05.288              0  \n",
       "4          0       4 2023-11-22 15:37:07.291              0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73b59cc6-c4c9-4acd-ad85-56ad3809763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa197550-ab9a-439d-a71f-62bab4524c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, beach: string, animal: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "\n",
    "animals = (\n",
    "    df\n",
    "    .select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\"))\n",
    "    .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\")\n",
    ")\n",
    "animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdce1d54-16e0-459b-aaf5-3ad2bd04ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 19:13:33 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>beach</th>\n",
       "      <th>animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>shark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>dolphin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key beach   animal\n",
       "0   C     C   turtle\n",
       "1   B     B    shark\n",
       "2   B     B  seagull\n",
       "3   B     B  seagull\n",
       "4   C     C  dolphin"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5c84669-d54e-479b-9ffa-ab4aef9c41d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 19:13:34 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13008"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e1e80dc-509f-4a6a-8c08-ca8b3d0d6508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c43444-81f8-472f-b81c-1040de0dd58b",
   "metadata": {},
   "source": [
    "# Streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d7ba406-af30-44b2-b0b4-a53f0a95be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source => transformations => sink\n",
    "# streaming_query = spark.readStream(????).????.writeStream(????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed6e1956-a3b3-43ee-bc55-b13ac280e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", broker)\n",
    "    .option(\"subscribe\", \"animals-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79e5ed09-6c6f-4b89-b3c9-f046f4de2456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87198d60-46b7-4a41-b038-5b5df8015219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, beach: string, animal: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "\n",
    "animals = (\n",
    "    df\n",
    "    .select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\"))\n",
    "    .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\")\n",
    ")\n",
    "animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c999e5e-e406-40bc-904c-e9b5c07865fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not supported for streaming\n",
    "# animals.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823de2a-39ac-4070-a021-27aaea5af87b",
   "metadata": {},
   "source": [
    "# Shark Alert App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc05ed91-8f39-473b-861f-907d26ac5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 19:13:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-29226a41-ec38-4538-8dcc-09eb664e7bf7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/22 19:13:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.streaming.query.StreamingQuery"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_query = (\n",
    "    animals\n",
    "    .filter(\"animal='shark'\")\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .outputMode(\"append\")\n",
    ").start()\n",
    "type(streaming_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "660e5254-037c-4c72-970f-30dff81e54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()\n",
    "# spark.streams.active[0].stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f3750-8f0d-4592-a1f2-128ae6900141",
   "metadata": {},
   "source": [
    "# Animal Counter App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a21fbea-6184-4e42-9b13-b6f14aaa46e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 19:23:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ce39832b-faf3-410a-a083-e919b5dc9e41. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/22 19:23:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/22 19:23:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark| 3505|\n",
      "|dolphin| 3527|\n",
      "|seagull| 3593|\n",
      "| turtle| 3532|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark| 3507|\n",
      "|seagull| 3593|\n",
      "|dolphin| 3530|\n",
      "| turtle| 3535|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark| 3510|\n",
      "|seagull| 3594|\n",
      "|dolphin| 3531|\n",
      "| turtle| 3540|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = (\n",
    "    animals.groupby(\"animal\").count()\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .outputMode(\"complete\")\n",
    ").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee0e0d51-782b-43c1-90cb-74329a02671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d487f-d303-490d-9476-c9ba303f92b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
