{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import station_pb2_grpc\n",
    "import station_pb2\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import grpc\n",
    "from datetime import date\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType,StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import corr\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Cassandra session\n"
     ]
    }
   ],
   "source": [
    "cluster = Cluster(['p6_db_2', 'p6_db_1', 'p6_db_3'])\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"cassandra-connector\")\n",
    "         .config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "         .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\")\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .config(\"spark.cassandra.connection.host\", \"p6_db_3,p6_db_2,p6_db_1\")  # Cassandra nodes\n",
    "         .config(\"spark.cassandra.connection.port\", \"9042\")  # Cassandra port\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cassandra_table():\n",
    "    # TODO: Q1\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE KEYSPACE weather WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}  AND durable_writes = true;\n",
      "CREATE TYPE weather.station_record (\n",
      "    tmin int,\n",
      "    tmax int\n",
      ");\n",
      "CREATE TABLE weather.stations (\n",
      "    id text,\n",
      "    date date,\n",
      "    name text static,\n",
      "    record frozen<station_record>,\n",
      "    PRIMARY KEY (id, date)\n",
      ") WITH CLUSTERING ORDER BY (date ASC)\n",
      "    AND additional_write_policy = '99p'\n",
      "    AND bloom_filter_fp_chance = 0.01\n",
      "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
      "    AND cdc = false\n",
      "    AND comment = ''\n",
      "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\n",
      "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
      "    AND crc_check_chance = 1.0\n",
      "    AND default_time_to_live = 0\n",
      "    AND extensions = {}\n",
      "    AND gc_grace_seconds = 864000\n",
      "    AND max_index_interval = 2048\n",
      "    AND memtable_flush_period_in_ms = 0\n",
      "    AND min_index_interval = 128\n",
      "    AND read_repair = 'BLOCKING'\n",
      "    AND speculative_retry = '99p';\n"
     ]
    }
   ],
   "source": [
    "# Q1 Ans\n",
    "setup_cassandra_table()\n",
    "print(session.execute(\"describe keyspace weather\").one().create_statement)\n",
    "print(session.execute(\"describe type weather.station_record\").one().create_statement)\n",
    "print(session.execute(\"describe table weather.stations\").one().create_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding metadata to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- ELEVATION: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- GSN FLAG: string (nullable = true)\n",
      " |-- HCN/CRN FLAG: string (nullable = true)\n",
      " |-- WMO ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = spark.read.option(\"header\", True).csv(\"/datasets/stations_metadata.csv\")\n",
    "table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Code to insert metadata into weather.stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313\n"
     ]
    }
   ],
   "source": [
    "# Metadata insert verify\n",
    "result = session.execute(\"SELECT COUNT(*) FROM weather.stations\")\n",
    "print(result.one()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(station_id):\n",
    "    # TODO: Q2\n",
    "    raise NotImplementedError() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row token: -1\n",
      "Vnode token: -1\n"
     ]
    }
   ],
   "source": [
    "# Q2 Ans\n",
    "row_token, vnode_token = get_tokens(\"USC00470273\")\n",
    "print(\"Row token:\", row_token)\n",
    "print(\"Vnode token:\", vnode_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Connect to the server\n",
    "channel = None\n",
    "stub = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sensor(sensor_id):\n",
    "    # TODO: gRPC client simulation\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gRPC client runner\n",
    "for station in [\"USW00014837\", \"USR0000WDDG\", \"USW00014898\", \"USW00014839\"]:\n",
    "    num_failed = simulate_sensor(station)\n",
    "    if num_failed > 0:\n",
    "        print(\"Failed to write all data for\", station)\n",
    "        break\n",
    "    \n",
    "    r = stub.StationMax(station_pb2.StationMaxRequest(station=station))\n",
    "    if r.error:\n",
    "        print(f\"error {r.error} in getting max temp of {station}\")\n",
    "    else:\n",
    "        print(f\"max temp for {station} is {r.tmax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandra_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"stations\", keyspace=\"weather\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_view():\n",
    "    # TODO: Initialize the weather2022 view\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather2022 verify\n",
    "create_weather_view()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Q3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Q5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
